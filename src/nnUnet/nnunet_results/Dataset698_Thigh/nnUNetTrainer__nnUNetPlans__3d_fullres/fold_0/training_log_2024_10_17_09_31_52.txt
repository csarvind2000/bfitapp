
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-10-17 09:31:52.758424: do_dummy_2d_data_aug: True 
2024-10-17 09:31:52.759510: Creating new 5-fold cross-validation split... 
2024-10-17 09:31:52.761510: Desired fold for training: 0 
2024-10-17 09:31:52.761581: This split has 143 training and 36 validation cases. 
2024-10-17 09:31:57.673970: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [16, 320, 320], 'median_image_size_in_voxels': [33.0, 640.0, 640.0], 'spacing': [5.0, 0.6875, 0.6875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset698_Thighcorrected', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [5.0, 0.6875, 0.6875], 'original_median_shape_after_transp': [33, 640, 640], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 200.0, 'mean': 190.4681854248047, 'median': 200.0, 'min': 0.0, 'percentile_00_5': 6.950581073760986, 'percentile_99_5': 200.0, 'std': 34.25586700439453}}} 
 
2024-10-17 09:31:57.691753: unpacking dataset... 
2024-10-17 09:32:15.310620: unpacking done... 
2024-10-17 09:32:15.313609: Unable to plot network architecture: nnUNet_compile is enabled! 
2024-10-17 09:32:17.724453:  
2024-10-17 09:32:17.724794: Epoch 0 
2024-10-17 09:32:17.724997: Current learning rate: 0.01 
2024-10-17 09:40:03.251011: train_loss -0.9072 
2024-10-17 09:40:03.251625: val_loss -0.9572 
2024-10-17 09:40:03.251773: Pseudo dice [np.float32(0.9891)] 
2024-10-17 09:40:03.251960: Epoch time: 465.53 s 
2024-10-17 09:40:03.252099: Yayy! New best EMA pseudo Dice: 0.9890999794006348 
2024-10-17 09:40:05.229477:  
2024-10-17 09:40:05.229747: Epoch 1 
2024-10-17 09:40:05.229937: Current learning rate: 0.0097 
2024-10-17 09:45:09.333285: train_loss -0.9567 
2024-10-17 09:45:09.333870: val_loss -0.9652 
2024-10-17 09:45:09.333985: Pseudo dice [np.float32(0.9907)] 
2024-10-17 09:45:09.334162: Epoch time: 304.11 s 
2024-10-17 09:45:09.334291: Yayy! New best EMA pseudo Dice: 0.989300012588501 
2024-10-17 09:45:13.692930:  
2024-10-17 09:45:13.693355: Epoch 2 
2024-10-17 09:45:13.693519: Current learning rate: 0.0094 
2024-10-17 09:50:04.998950: train_loss -0.9634 
2024-10-17 09:50:04.999305: val_loss -0.9661 
2024-10-17 09:50:04.999470: Pseudo dice [np.float32(0.9914)] 
2024-10-17 09:50:04.999653: Epoch time: 291.31 s 
2024-10-17 09:50:04.999754: Yayy! New best EMA pseudo Dice: 0.9894999861717224 
2024-10-17 09:50:09.260138:  
2024-10-17 09:50:09.260486: Epoch 3 
2024-10-17 09:50:09.260657: Current learning rate: 0.0091 
2024-10-17 09:54:56.453939: train_loss -0.955 
2024-10-17 09:54:56.454356: val_loss -0.9609 
2024-10-17 09:54:56.454540: Pseudo dice [np.float32(0.9902)] 
2024-10-17 09:54:56.454693: Epoch time: 287.2 s 
2024-10-17 09:54:56.454780: Yayy! New best EMA pseudo Dice: 0.9896000027656555 
2024-10-17 09:55:00.923193:  
2024-10-17 09:55:00.923328: Epoch 4 
2024-10-17 09:55:00.923512: Current learning rate: 0.00879 
2024-10-17 09:59:48.385890: train_loss -0.9627 
2024-10-17 09:59:48.386399: val_loss -0.9668 
2024-10-17 09:59:48.386486: Pseudo dice [np.float32(0.9916)] 
2024-10-17 09:59:48.386615: Epoch time: 287.46 s 
2024-10-17 09:59:48.386709: Yayy! New best EMA pseudo Dice: 0.989799976348877 
2024-10-17 09:59:52.700983:  
2024-10-17 09:59:52.701390: Epoch 5 
2024-10-17 09:59:52.701572: Current learning rate: 0.00849 
2024-10-17 10:04:48.657481: train_loss -0.9585 
2024-10-17 10:04:48.657866: val_loss -0.9648 
2024-10-17 10:04:48.657980: Pseudo dice [np.float32(0.9904)] 
2024-10-17 10:04:48.658136: Epoch time: 295.96 s 
2024-10-17 10:04:48.658305: Yayy! New best EMA pseudo Dice: 0.989799976348877 
2024-10-17 10:04:52.956310:  
2024-10-17 10:04:52.956698: Epoch 6 
2024-10-17 10:04:52.956853: Current learning rate: 0.00818 
2024-10-17 10:09:56.247187: train_loss -0.961 
2024-10-17 10:09:56.247705: val_loss -0.9644 
2024-10-17 10:09:56.248034: Pseudo dice [np.float32(0.9907)] 
2024-10-17 10:09:56.248175: Epoch time: 303.29 s 
2024-10-17 10:09:56.248286: Yayy! New best EMA pseudo Dice: 0.9898999929428101 
2024-10-17 10:10:00.714033:  
2024-10-17 10:10:00.714443: Epoch 7 
2024-10-17 10:10:00.714633: Current learning rate: 0.00787 
2024-10-17 10:15:02.950956: train_loss -0.968 
2024-10-17 10:15:02.951737: val_loss -0.9699 
2024-10-17 10:15:02.951846: Pseudo dice [np.float32(0.9916)] 
2024-10-17 10:15:02.951990: Epoch time: 302.24 s 
2024-10-17 10:15:02.952099: Yayy! New best EMA pseudo Dice: 0.9901000261306763 
2024-10-17 10:15:07.334331:  
2024-10-17 10:15:07.334753: Epoch 8 
2024-10-17 10:15:07.335055: Current learning rate: 0.00756 
2024-10-17 10:20:08.019383: train_loss -0.9689 
2024-10-17 10:20:08.019707: val_loss -0.971 
2024-10-17 10:20:08.019831: Pseudo dice [np.float32(0.9918)] 
2024-10-17 10:20:08.020037: Epoch time: 300.69 s 
2024-10-17 10:20:08.020143: Yayy! New best EMA pseudo Dice: 0.9902999997138977 
2024-10-17 10:20:12.515937:  
2024-10-17 10:20:12.516272: Epoch 9 
2024-10-17 10:20:12.516465: Current learning rate: 0.00725 
2024-10-17 10:25:05.492304: train_loss -0.9661 
2024-10-17 10:25:05.492739: val_loss -0.9708 
2024-10-17 10:25:05.492846: Pseudo dice [np.float32(0.9921)] 
2024-10-17 10:25:05.492958: Epoch time: 292.98 s 
2024-10-17 10:25:05.493039: Yayy! New best EMA pseudo Dice: 0.9904000163078308 
2024-10-17 10:25:09.944787:  
2024-10-17 10:25:09.945152: Epoch 10 
2024-10-17 10:25:09.945348: Current learning rate: 0.00694 
2024-10-17 10:29:58.116657: train_loss -0.9606 
2024-10-17 10:29:58.117156: val_loss -0.9731 
2024-10-17 10:29:58.117355: Pseudo dice [np.float32(0.9922)] 
2024-10-17 10:29:58.117467: Epoch time: 288.17 s 
2024-10-17 10:29:58.117549: Yayy! New best EMA pseudo Dice: 0.9905999898910522 
2024-10-17 10:30:02.472928:  
2024-10-17 10:30:02.473386: Epoch 11 
2024-10-17 10:30:02.473570: Current learning rate: 0.00663 
2024-10-17 10:34:48.759969: train_loss -0.9693 
2024-10-17 10:34:48.760240: val_loss -0.971 
2024-10-17 10:34:48.760358: Pseudo dice [np.float32(0.9919)] 
2024-10-17 10:34:48.760544: Epoch time: 286.29 s 
2024-10-17 10:34:48.760650: Yayy! New best EMA pseudo Dice: 0.9907000064849854 
2024-10-17 10:34:52.933458:  
2024-10-17 10:34:52.933763: Epoch 12 
2024-10-17 10:34:52.933922: Current learning rate: 0.00631 
2024-10-17 10:39:39.030110: train_loss -0.9608 
2024-10-17 10:39:39.030511: val_loss -0.9699 
2024-10-17 10:39:39.030685: Pseudo dice [np.float32(0.9914)] 
2024-10-17 10:39:39.030885: Epoch time: 286.1 s 
2024-10-17 10:39:39.031000: Yayy! New best EMA pseudo Dice: 0.9908000230789185 
2024-10-17 10:39:43.434242:  
2024-10-17 10:39:43.434541: Epoch 13 
2024-10-17 10:39:43.434725: Current learning rate: 0.006 
2024-10-17 10:44:29.545753: train_loss -0.9666 
2024-10-17 10:44:29.546364: val_loss -0.9677 
2024-10-17 10:44:29.546504: Pseudo dice [np.float32(0.9907)] 
2024-10-17 10:44:29.546634: Epoch time: 286.11 s 
2024-10-17 10:44:31.026152:  
2024-10-17 10:44:31.026627: Epoch 14 
2024-10-17 10:44:31.026780: Current learning rate: 0.00568 
2024-10-17 10:49:17.383380: train_loss -0.9702 
2024-10-17 10:49:17.383766: val_loss -0.9725 
2024-10-17 10:49:17.383954: Pseudo dice [np.float32(0.9922)] 
2024-10-17 10:49:17.384144: Epoch time: 286.36 s 
2024-10-17 10:49:17.384269: Yayy! New best EMA pseudo Dice: 0.9908999800682068 
2024-10-17 10:49:21.764242:  
2024-10-17 10:49:21.764514: Epoch 15 
2024-10-17 10:49:21.764887: Current learning rate: 0.00536 
2024-10-17 10:54:07.898019: train_loss -0.9678 
2024-10-17 10:54:07.898278: val_loss -0.9734 
2024-10-17 10:54:07.898382: Pseudo dice [np.float32(0.9924)] 
2024-10-17 10:54:07.898506: Epoch time: 286.14 s 
2024-10-17 10:54:07.898586: Yayy! New best EMA pseudo Dice: 0.991100013256073 
2024-10-17 10:54:12.234640:  
2024-10-17 10:54:12.235044: Epoch 16 
2024-10-17 10:54:12.235172: Current learning rate: 0.00504 
2024-10-17 10:58:58.815979: train_loss -0.9679 
2024-10-17 10:58:58.816504: val_loss -0.9722 
2024-10-17 10:58:58.816622: Pseudo dice [np.float32(0.9917)] 
2024-10-17 10:58:58.816749: Epoch time: 286.58 s 
2024-10-17 10:58:58.816920: Yayy! New best EMA pseudo Dice: 0.991100013256073 
2024-10-17 10:59:03.173466:  
2024-10-17 10:59:03.173774: Epoch 17 
2024-10-17 10:59:03.173954: Current learning rate: 0.00471 
2024-10-17 11:03:49.571871: train_loss -0.9736 
2024-10-17 11:03:49.572445: val_loss -0.973 
2024-10-17 11:03:49.572539: Pseudo dice [np.float32(0.992)] 
2024-10-17 11:03:49.572651: Epoch time: 286.4 s 
2024-10-17 11:03:49.572756: Yayy! New best EMA pseudo Dice: 0.9911999702453613 
2024-10-17 11:03:53.941583:  
2024-10-17 11:03:53.941943: Epoch 18 
2024-10-17 11:03:53.942253: Current learning rate: 0.00438 
2024-10-17 11:08:40.275347: train_loss -0.9712 
2024-10-17 11:08:40.275782: val_loss -0.9689 
2024-10-17 11:08:40.276055: Pseudo dice [np.float32(0.9913)] 
2024-10-17 11:08:40.276190: Epoch time: 286.34 s 
2024-10-17 11:08:40.276293: Yayy! New best EMA pseudo Dice: 0.9911999702453613 
2024-10-17 11:08:44.743976:  
2024-10-17 11:08:44.744547: Epoch 19 
2024-10-17 11:08:44.744812: Current learning rate: 0.00405 
2024-10-17 11:13:37.943607: train_loss -0.9727 
2024-10-17 11:13:37.943993: val_loss -0.9683 
2024-10-17 11:13:37.944121: Pseudo dice [np.float32(0.9911)] 
2024-10-17 11:13:37.944251: Epoch time: 293.2 s 
2024-10-17 11:13:39.574813:  
2024-10-17 11:13:39.575289: Epoch 20 
2024-10-17 11:13:39.575462: Current learning rate: 0.00372 
2024-10-17 11:18:47.508124: train_loss -0.9708 
2024-10-17 11:18:47.508521: val_loss -0.9718 
2024-10-17 11:18:47.508780: Pseudo dice [np.float32(0.992)] 
2024-10-17 11:18:47.508943: Epoch time: 307.94 s 
2024-10-17 11:18:47.509045: Yayy! New best EMA pseudo Dice: 0.9912999868392944 
2024-10-17 11:18:52.043123:  
2024-10-17 11:18:52.043507: Epoch 21 
2024-10-17 11:18:52.043754: Current learning rate: 0.00338 
2024-10-17 11:23:42.917834: train_loss -0.9741 
2024-10-17 11:23:42.918117: val_loss -0.9735 
2024-10-17 11:23:42.918313: Pseudo dice [np.float32(0.9921)] 
2024-10-17 11:23:42.918438: Epoch time: 290.88 s 
2024-10-17 11:23:42.918537: Yayy! New best EMA pseudo Dice: 0.9914000034332275 
2024-10-17 11:23:47.766993:  
2024-10-17 11:23:47.767561: Epoch 22 
2024-10-17 11:23:47.767751: Current learning rate: 0.00304 
2024-10-17 11:28:36.259217: train_loss -0.9706 
2024-10-17 11:28:36.259529: val_loss -0.9772 
2024-10-17 11:28:36.259629: Pseudo dice [np.float32(0.9928)] 
2024-10-17 11:28:36.259754: Epoch time: 288.49 s 
2024-10-17 11:28:36.259984: Yayy! New best EMA pseudo Dice: 0.9915000200271606 
2024-10-17 11:28:40.796799:  
2024-10-17 11:28:40.797235: Epoch 23 
2024-10-17 11:28:40.797418: Current learning rate: 0.0027 
2024-10-17 11:33:40.623770: train_loss -0.973 
2024-10-17 11:33:40.624191: val_loss -0.9745 
2024-10-17 11:33:40.624309: Pseudo dice [np.float32(0.9924)] 
2024-10-17 11:33:40.624441: Epoch time: 299.83 s 
2024-10-17 11:33:40.624550: Yayy! New best EMA pseudo Dice: 0.991599977016449 
2024-10-17 11:33:45.209064:  
2024-10-17 11:33:45.209516: Epoch 24 
2024-10-17 11:33:45.209741: Current learning rate: 0.00235 
2024-10-17 11:38:33.835913: train_loss -0.9719 
2024-10-17 11:38:33.836350: val_loss -0.9713 
2024-10-17 11:38:33.836556: Pseudo dice [np.float32(0.992)] 
2024-10-17 11:38:33.836678: Epoch time: 288.63 s 
2024-10-17 11:38:33.836766: Yayy! New best EMA pseudo Dice: 0.991599977016449 
2024-10-17 11:38:38.378602:  
2024-10-17 11:38:38.378973: Epoch 25 
2024-10-17 11:38:38.379162: Current learning rate: 0.00199 
2024-10-17 11:43:28.209316: train_loss -0.9739 
2024-10-17 11:43:28.209752: val_loss -0.9738 
2024-10-17 11:43:28.209874: Pseudo dice [np.float32(0.9923)] 
2024-10-17 11:43:28.209987: Epoch time: 289.83 s 
2024-10-17 11:43:28.210068: Yayy! New best EMA pseudo Dice: 0.9916999936103821 
2024-10-17 11:43:32.862411:  
2024-10-17 11:43:32.862806: Epoch 26 
2024-10-17 11:43:32.862939: Current learning rate: 0.00163 
2024-10-17 11:48:24.408298: train_loss -0.9742 
2024-10-17 11:48:24.408954: val_loss -0.9761 
2024-10-17 11:48:24.409086: Pseudo dice [np.float32(0.9927)] 
2024-10-17 11:48:24.409225: Epoch time: 291.55 s 
2024-10-17 11:48:24.409345: Yayy! New best EMA pseudo Dice: 0.9918000102043152 
2024-10-17 11:48:29.007613:  
2024-10-17 11:48:29.007963: Epoch 27 
2024-10-17 11:48:29.008123: Current learning rate: 0.00126 
2024-10-17 11:53:19.138845: train_loss -0.9738 
2024-10-17 11:53:19.139394: val_loss -0.9733 
2024-10-17 11:53:19.139493: Pseudo dice [np.float32(0.9919)] 
2024-10-17 11:53:19.139711: Epoch time: 290.13 s 
2024-10-17 11:53:19.139827: Yayy! New best EMA pseudo Dice: 0.9918000102043152 
2024-10-17 11:53:23.997765:  
2024-10-17 11:53:23.998157: Epoch 28 
2024-10-17 11:53:23.998477: Current learning rate: 0.00087 
2024-10-17 11:58:23.263966: train_loss -0.9744 
2024-10-17 11:58:23.264297: val_loss -0.9745 
2024-10-17 11:58:23.264413: Pseudo dice [np.float32(0.9928)] 
2024-10-17 11:58:23.264529: Epoch time: 299.27 s 
2024-10-17 11:58:23.264611: Yayy! New best EMA pseudo Dice: 0.9919000267982483 
2024-10-17 11:58:27.780755:  
2024-10-17 11:58:27.781122: Epoch 29 
2024-10-17 11:58:27.781360: Current learning rate: 0.00047 
2024-10-17 12:03:18.568983: train_loss -0.9739 
2024-10-17 12:03:18.569396: val_loss -0.9712 
2024-10-17 12:03:18.569507: Pseudo dice [np.float32(0.9916)] 
2024-10-17 12:03:18.569715: Epoch time: 290.79 s 
2024-10-17 12:03:20.648574: Training done. 
2024-10-17 12:03:20.663700: Using splits from existing split file: /media/tct-bii/DataHDD/abdomen_fat/nnUNet/nnUNet/nnUNet_preprocessed/Dataset698_Thighcorrected/splits_final.json 
2024-10-17 12:03:20.664088: The split file contains 5 splits. 
2024-10-17 12:03:20.664168: Desired fold for training: 0 
2024-10-17 12:03:20.664235: This split has 143 training and 36 validation cases. 
2024-10-17 12:03:20.664551: predicting Subject110 
2024-10-17 12:03:20.665690: Subject110, shape torch.Size([1, 26, 640, 639]), rank 0 
2024-10-17 12:04:04.679310: predicting Subject117 
2024-10-17 12:04:04.685093: Subject117, shape torch.Size([1, 31, 640, 640]), rank 0 
2024-10-17 12:04:40.195506: predicting Subject118 
2024-10-17 12:04:40.197879: Subject118, shape torch.Size([1, 31, 640, 640]), rank 0 
2024-10-17 12:05:15.597669: predicting Subject124 
2024-10-17 12:05:15.600305: Subject124, shape torch.Size([1, 25, 640, 640]), rank 0 
2024-10-17 12:05:50.916775: predicting Subject133 
2024-10-17 12:05:50.919143: Subject133, shape torch.Size([1, 23, 640, 639]), rank 0 
2024-10-17 12:06:14.755955: predicting Subject135 
2024-10-17 12:06:14.758794: Subject135, shape torch.Size([1, 30, 640, 640]), rank 0 
2024-10-17 12:06:50.264532: predicting Subject140 
2024-10-17 12:06:50.267059: Subject140, shape torch.Size([1, 35, 640, 639]), rank 0 
2024-10-17 12:07:38.189565: predicting Subject142 
2024-10-17 12:07:38.193253: Subject142, shape torch.Size([1, 32, 640, 640]), rank 0 
2024-10-17 12:08:13.990846: predicting Subject148 
2024-10-17 12:08:13.993888: Subject148, shape torch.Size([1, 26, 640, 640]), rank 0 
2024-10-17 12:08:49.067347: predicting Subject150 
2024-10-17 12:08:49.071451: Subject150, shape torch.Size([1, 26, 640, 639]), rank 0 
2024-10-17 12:09:24.798927: predicting Subject155 
2024-10-17 12:09:24.802665: Subject155, shape torch.Size([1, 31, 640, 640]), rank 0 
2024-10-17 12:10:00.369714: predicting Subject16 
2024-10-17 12:10:00.374033: Subject16, shape torch.Size([1, 39, 582, 582]), rank 0 
2024-10-17 12:10:48.374180: predicting Subject166 
2024-10-17 12:10:48.377481: Subject166, shape torch.Size([1, 36, 640, 640]), rank 0 
2024-10-17 12:11:35.883763: predicting Subject168 
2024-10-17 12:11:35.888340: Subject168, shape torch.Size([1, 39, 640, 639]), rank 0 
2024-10-17 12:12:22.907169: predicting Subject176 
2024-10-17 12:12:22.911889: Subject176, shape torch.Size([1, 33, 640, 640]), rank 0 
2024-10-17 12:13:10.296152: predicting Subject186 
2024-10-17 12:13:10.300652: Subject186, shape torch.Size([1, 36, 640, 639]), rank 0 
2024-10-17 12:13:57.806463: predicting Subject190 
2024-10-17 12:13:57.811736: Subject190, shape torch.Size([1, 35, 669, 669]), rank 0 
2024-10-17 12:15:21.035335: predicting Subject194 
2024-10-17 12:15:21.038965: Subject194, shape torch.Size([1, 36, 640, 640]), rank 0 
2024-10-17 12:16:08.616887: predicting Subject196 
2024-10-17 12:16:08.621322: Subject196, shape torch.Size([1, 30, 640, 640]), rank 0 
2024-10-17 12:16:44.078196: predicting Subject198 
2024-10-17 12:16:44.082433: Subject198, shape torch.Size([1, 28, 640, 639]), rank 0 
2024-10-17 12:17:19.810707: predicting Subject23 
2024-10-17 12:17:19.813690: Subject23, shape torch.Size([1, 33, 582, 582]), rank 0 
2024-10-17 12:18:06.616544: predicting Subject41 
2024-10-17 12:18:06.618862: Subject41, shape torch.Size([1, 25, 582, 582]), rank 0 
2024-10-17 12:18:42.455964: predicting Subject45 
2024-10-17 12:18:42.459149: Subject45, shape torch.Size([1, 31, 640, 640]), rank 0 
2024-10-17 12:19:17.935936: predicting Subject50 
2024-10-17 12:19:17.939868: Subject50, shape torch.Size([1, 36, 684, 684]), rank 0 
2024-10-17 12:20:44.851768: predicting Subject56 
2024-10-17 12:20:44.854944: Subject56, shape torch.Size([1, 37, 640, 640]), rank 0 
2024-10-17 12:21:32.562932: predicting Subject66 
2024-10-17 12:21:32.566693: Subject66, shape torch.Size([1, 39, 640, 640]), rank 0 
2024-10-17 12:22:20.455800: predicting Subject69 
2024-10-17 12:22:20.459775: Subject69, shape torch.Size([1, 31, 640, 639]), rank 0 
2024-10-17 12:22:57.474164: predicting Subject72 
2024-10-17 12:22:57.477015: Subject72, shape torch.Size([1, 33, 640, 640]), rank 0 
2024-10-17 12:23:45.086962: predicting Subject73 
2024-10-17 12:23:45.089695: Subject73, shape torch.Size([1, 31, 640, 640]), rank 0 
2024-10-17 12:24:20.589647: predicting Subject75 
2024-10-17 12:24:20.593009: Subject75, shape torch.Size([1, 34, 640, 638]), rank 0 
2024-10-17 12:25:08.124828: predicting Subject80 
2024-10-17 12:25:08.128443: Subject80, shape torch.Size([1, 29, 640, 640]), rank 0 
2024-10-17 12:25:43.285813: predicting Subject81 
2024-10-17 12:25:43.289609: Subject81, shape torch.Size([1, 30, 640, 640]), rank 0 
2024-10-17 12:26:18.964987: predicting Subject89 
2024-10-17 12:26:18.967894: Subject89, shape torch.Size([1, 32, 640, 640]), rank 0 
2024-10-17 12:26:54.420820: predicting Subject94 
2024-10-17 12:26:54.425162: Subject94, shape torch.Size([1, 32, 655, 655]), rank 0 
2024-10-17 12:27:56.435644: predicting Subject95 
2024-10-17 12:27:56.439323: Subject95, shape torch.Size([1, 34, 640, 640]), rank 0 
2024-10-17 12:28:42.994120: predicting Subject98 
2024-10-17 12:28:42.996878: Subject98, shape torch.Size([1, 31, 640, 640]), rank 0 
2024-10-17 12:29:31.991249: Validation complete 
2024-10-17 12:29:31.991386: Mean Validation Dice:  0.9914837578083591 
